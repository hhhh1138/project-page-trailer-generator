<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>An Inverse Partial Optimal Transport Framework for Music-guided Movie Trailer Generation</title>
<link href="./DreamBooth_files/style.css" rel="stylesheet">
<script type="text/javascript" src="./DreamBooth_files/jquery.mlens-1.0.min.js"></script> 
<script type="text/javascript" src="./DreamBooth_files/jquery.js"></script>
</head>

<body>
<div class="content">
  <h1><strong>An Inverse Partial Optimal Transport Framework for Music-guided Movie Trailer Generation</strong></h1>
  <p id="authors"><span><a href="https://natanielruiz.github.io/"></a></span><a href="https://github.com/hhhh1138">Yutong Wang<sup>1*</sup></a> <a href="https://github.com/Amber0913">Sidan Zhu<sup>1*</sup></a> <a href="https://hongtengxu.github.io/">Hongteng Xu<sup>2</sup></a> <a href="https://dixinluo.github.io/">Dixin Luo<sup>1✝</sup></a><br>
   <br>
  <span style="font-size: 16px"><sup>*</sup>Equal Contribution. &nbsp;&nbsp; <sup>✝</sup>Corresponding Author.</span> 
   <br>
  <span style="font-size: 18px"><sup>1</sup>Beijing Institute of Technology &nbsp;&nbsp; <sup>2</sup>Renmin University of China</span>
    </p>
  <br>
<!--   <img src="./DreamBooth_files/teaser_static.jpg" class="teaser-gif" style="width:100%;"><br>
  <h3 style="text-align:center"><em>It’s like a photo booth, but once the subject is captured, it can be synthesized wherever your dreams take you…</em></h3>
    <font size="+2">
          <p style="text-align: center;">
            <a href="https://arxiv.org/abs/2208.12242" target="_blank">[Paper]</a> &nbsp;&nbsp;&nbsp;&nbsp;
	    (<font color="#C70039">new!</font>) <a href="https://github.com/google/dreambooth" target="_blank">[Dataset]</a> &nbsp;&nbsp;&nbsp;&nbsp;
            <a href="DreamBooth_files/bibtex.txt" target="_blank">[BibTeX]</a>
          </p>
    </font> -->
</div>
<div class="content">
  <h2 style="text-align:center;">Abstract</h2>
  <p>Trailer generation is a challenging video clipping task that aims to select highlighting shots from long videos like movies and re-organize them in an attractive way. 
In this study, we propose an inverse partial optimal transport (IPOT) framework to achieve music-guided movie trailer generation. 
In particular, we formulate the trailer generation task as selecting and sorting key movie shots based on audio shots, which involves matching the latent representations across visual and acoustic modalities. 
We learn a multi-modal latent representation model in the proposed IPOT framework to achieve this aim. 
In this framework, a two-tower encoder derives the latent representations of movie and music shots, respectively, and an attention-assisted Sinkhorn matching network parameterizes the grounding distance between the shots' latent representations and the distribution of the movie shots. 
Taking the correspondence between the movie shots and its trailer music shots as the observed optimal transport plan defined on the grounding distances, we learn the model by solving an inverse partial optimal transport problem, leading to a bi-level optimization strategy. 
We collect real-world movies and their trailers to construct a dataset with abundant label information called CMTD and, accordingly, train and evaluate various automatic trailer generators. 
Compared with state-of-the-art methods, our IPOT method consistently shows superiority in subjective visual effects and objective quantitative measurements.</p>
</div>
	
<!-- <div class="content">
  <h2>Background</h2>
  <p> Given a particular subject such as clock (shown in the real images on the left), it is very challenging to generate it in different contexts with state-of-the-art text-to-image models, while maintaining high fidelity to its key visual features. Even with dozens of iterations over a text prompt that contains a detailed description of the appearance of the clock (<em>"retro style yellow alarm clock with a white clock face and a yellow number three on the right part of the clock face in the jungle"</em>), the Imagen model [Saharia et al., 2022] can't reconstruct its key visual features (third column). Furthermore, even models whose text embedding lies in a shared language-vision space and can create semantic variations of the image, such as DALL-E2 [Ramesh et al., 2022], can neither reconstruct the appearance of the given subject nor modify the context (second column). In contrast, our approach (right) can synthesize the clock with high fidelity and in new contexts (<em>"a [V] clock in the jungle"</em>).</p>
  <br>
  <img class="summary-img" src="./DreamBooth_files/background.png" style="width:100%;"> <br>
</div> -->
	
<div class="content">
  <h2>Approach</h2>
  <img class="summary-img" src="./imgs/ipot_schemes.png" style="width:100%;"> <br>
</div>
	
<div class="content">
  <h2>Dataset</h2>
  <p>...</p>
<img class="summary-img" src="./imgs/dataset.png" style="width:100%;">
</div>
	
<div class="content">
  <h2>Gallery</h2>
  <img class="summary-img" src="./imgs/visualization1.png" style="width:100%;"> <br>
  <img class="summary-img" src="./imgs/visualization2.png" style="width:100%;"> <br>
  <p> Comparisons between some generated trailers of various methods and the official trailer. </p>
  <br>
<!--   <img class="summary-img" src="./DreamBooth_files/art.png" style="width:100%;"> <br> -->
</div>
	

<div class="content">
  <h2>BibTex</h2>
<!--   <code> @article{ruiz2022dreambooth,<br>
  &nbsp;&nbsp;title={DreamBooth: Fine Tuning Text-to-image Diffusion Models for Subject-Driven Generation},<br>
  &nbsp;&nbsp;author={Ruiz, Nataniel and Li, Yuanzhen and Jampani, Varun and Pritch, Yael and Rubinstein, Michael and Aberman, Kfir},<br>
  &nbsp;&nbsp;booktitle={arXiv preprint arxiv:2208.12242},<br>
  &nbsp;&nbsp;year={2022}<br>
  } </code>  -->
</div>
<div class="content" id="acknowledgements">
  <p>
	  The project page template is borrowed from <a href="https://dreambooth.github.io/">DreamBooth</a>.
  </p>
</div>
</body>
</html>
